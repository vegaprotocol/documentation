---
sidebar_position: 3
title: Set up a data node
hide_title: false
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

To set up a data node, you must first have followed the guide to [install and set up a Vega node](setup-server.md).

A data node must be run in conjunction with a Vega node. The Vega node will send the events it receives from the network and the events it creates to the data node, which will then store them in a database. An API is provided to query the data stored by the data node.

The database used by the data node is a PostgreSQL database with the Timescale extension installed. The database can be a dedicated database server, a docker container, or an embedded version of PostgreSql with Timescale installed that is provided by Vega.

:::note Operating system 
The following instructions assume you are installing on a Ubuntu Linux machine as explained in the [server setup guide](setup-server#os-and-software).
:::

## Pre-requisites

### Vega core
Please follow the instructions in the [server setup guide](setup-server.md) to install Vega.

### PostgreSQL and TimescaleDB full installation
We have tested and recommend using version 2.8.0 of the TimescaleDB plugin with Postgres 14. 

Refer to the [PostgreSQL documentation ↗](https://www.postgresql.org/docs/14/index.html) for more detailed information on setting up a PostgreSQL database.

:::note Linux and MacOS guides

<Tabs groupId="operating-systems">
<TabItem value="linux" label="Linux">

### Linux users
To ensure you install the correct version of TimescaleDB, you can use the notes at the bottom of the [Timescales Documentation for Debian ↗](https://docs.timescale.com/install/latest/self-hosted/installation-debian/).

</TabItem>
<TabItem value="mac" label="MacOS">

### MacOS users
To ensure you install the correct version of TimescaleDB, you can use the notes at the bottom of the [Timescales Documentation for Mac ↗](https://docs.timescale.com/install/latest/self-hosted/installation-macos/). 
</TabItem>
</Tabs>
:::

:::note Database user roles
Due to operations that are required for snapshotting, the database user for the data-node must have superuser privileges. This is a limitation that we currently have due to Postgresql 14 and TimescaleDB.
:::

### PostgreSQL and TimescaleDB docker installation
If you prefer to run PostgreSQL and TimescaleDB in a docker container, you can use the following command to start a Postgresql docker container with TimescaleDB installed:

This guide assumes you already have Docker installed on your system. For full installation guide consult Docker's [documentation ↗](https://docs.docker.com/engine/install/ubuntu/).

```Shell
docker run -d \
    --rm \
    --name MY_LOVELY_DB_CONTAINER \
    -e POSTGRES_USER=DATABASE_USER \
    -e POSTGRES_PASSWORD=DATABASE_PASSWORD \
    -e POSTGRES_DB=DATABASE_NAME \
    -p LOCALDB_PORT:5432
    -v /host_path/to/snapshotsCopyTo:/snapshotsCopyTo:z \
    -v /host_path/to/snapshotsCopyFrom:/snapshotsCopyFrom:z \
    timescale/timescaledb:2.8.0-pg14
```

Where:

- `database_user` is the user name you want to use to connect to the database.
- `database_password` is the password you want to use to connect to the database.
- `database_name` is the name of the database you want to use for storing the data.
- `localdb_port` is the port you want to use to connect to the database on your local machine. (5432 is the default port for Postgresql database server and may not be available if you already have a postgresql database server running on your machine and want to use Docker for testing).
- `/host_path/to/snapshotsCopyTo` is the path on your host machine where you want to store the snapshots that are generated by data node. Snapshots allow you to restore the database to a previous state.
- `/host_path/to/snapshotsCopyFrom` is the path on your host machine where you want to store the snapshots that are retrieved from IPFS and can be used to rebuild a data node database from another data node.

### PostgreSQL configuration tuning

The default PostgreSQL configuration is not optimised for memory usage, and can be modified. 

Find the PostgreSQL parameters in the `postgresq.conf` file. The default file path for Linux and PostgreSQL 14 is: `/etc/postgresql/14/main/postgresql.conf`.

:::note Memory usage
Total memory usage for PostgreSQL is predictable. To determine the values of the parameters below, you must know how PostgreSQL uses the memory. 
There is a `shared_memory` that is used between all connections and background workers. 

Each background worker and connection has its own smaller chunk of memory:

- `work_mem` - memory available for the query buffers in the connection session.
- `temp_buffers` - memory available for accessing temporary tables by the connection session.

You can assume that `Max RAM` utilisation can be rounded to: `shared_buffer + (temp_buffers + work_mem) * max_connections`.
:::

The suggested parameters are below.

#### Max connections

New value:

```conf
max_connections = 50
```

Limiting the maximum number of connections reduces the memory usage by PostgreSQL.

#### Huge pages

New value:

```conf
huge_pages = off
```

The default value of the `huge_pages` config is `try`. Setting it to `off` usually reduces the RAM usage, however, it increases the CPU usage on the machine.

#### Work mem

New value:

```conf
work_mem = 5MB
```

#### Temp buffers

New value:

```conf
temp_buffers = 5MB
```

#### Shared buffers

New value:

```conf
shared_buffers = 2GB
```

This value should be set to 25% of your server’s physical memory. The 2GB value would work for a server with 8GB physical memory.

#### Dynamic shared memory type

New value:

```conf
dynamic_shared_memory_type = sysv
```

#### Shared memory type

New value:

```conf
shared_memory_type = sysv
```

The two above parameters determine how your operating system manages the shared memory.

If your operating system supports the POSIX standard, you may want to use the `map` value both for the `dynamic_shared_memory_type` and `shared_memory_type`. But the `sysv` value is more portable than `map`. There is no significant difference in [performance ↗](https://lists.dragonflybsd.org/pipermail/kernel/attachments/20120913/317c1aab/attachment-0001.pdf). 

## Generate configuration files

### Vega and Tendermint configuration
Before you can use Vega, you need to generate the default configuration files for Vega and Tendermint. You can then alter those to the specific requirements.

The below command will create home paths (if they don't already exist) and generate the configuration in the paths you chose.

```shell
vega init --home="YOUR_VEGA_HOME_PATH" --tendermint-home="YOUR_TENDERMINT_HOME_PATH" full
```

To update your node configuration, such as to set up ports for the APIs, edit the config file:

```shell
"YOUR_VEGA_HOME_PATH"/config/node/config.toml
```

:::note For more information about setting up a validator node, see the [validator node setup guide](setup-validator.md).
:::

### Data node configuration

## Data node retention profiles
When starting a data node, you can choose the data retention configuration for your data node, depending on the use case for the node. The retention policy details can all be fine-tuned manually, as well.

There are 3 retention policy configurations: 
* **Standard (default)**: The node retains data according to the default retention policies, which assume a data node retains some data over time, but not all data
* **Lite**: The node retains enough data to be able to provide the latest state to clients, and produce network history segments. This mode saves enough to provide the current state of accounts, assets, balances, delegations, liquidity provisions, live orders, margin levels, markets, network limits, network parameters, node details, parties, positions 
* **Archive**: The node retains all data

To run a node that doesn't use the standard default retention, use one of the following flags when running the `init` command: 
   
* For a standard node, no flag
* For an archive node, use `--archive`
* For a lite node, use `--lite`

If you want to tweak the retention policy once the initial configuration has been generated, set it on per-table basis in the data node's `config.toml`. 

For example: 

```toml
[[SQLStore.RetentionPolicies]]
  HypertableOrCaggName = "balances"
  DataRetentionPeriod = "7 days"
## Generate config
To generate the configuration files you need for the data node, you can use the following command:

```shell
vega datanode init --home="YOUR_DATA_NODE_HOME_PATH" "CHAIN_ID"
```
Find the `CHAIN_ID` by going to the relevant network genesis file in the relevant networks repo. 

Visit [networks ↗](https://github.com/vegaprotocol/networks/) for mainnet or [networks-internal ↗](https://github.com/vegaprotocol/networks-internal) for a testnet network. 

To update your data node configuration, such as to set up ports for the APIs or database credentials, edit the config file:

```shell
"YOUR_DATA_NODE_HOME_PATH"/config/data-node/config.toml
```

## Configure nodes

### Vega
To configure your Vega node to work with a data node you need to update the `[Broker.Socket]` section of the Vega configuration file `YOUR_VEGA_HOME_PATH/config/node/config.toml` from false to:

```toml
  [Broker.Socket]
    ...
    Enabled = true
    ...
```

:::note 
While it's possible to run the data node and Vega node on separate machines, it's not recommended given the volume of data that will be transferred between the two.
:::

### Data node database
Data node database configuration is defined under the `[SQLStore.ConnectionConfig]` section of the data node configuration file `YOUR_DATA_NODE_HOME_PATH/config/data-node/config.toml`:

```toml
  [SQLStore.ConnectionConfig]
    Host = "localhost"
    Port = 5432
    Username = "vega"
    Password = "vega"
    Database = "vega"
    MaxConnLifetime = "30m0s"
    MaxConnLifetimeJitter = "5m0s"
```

You should ensure the database configuration matches those of the database you created in the pre-requisite steps.

### Wipe on startup
:::warning Database wipe
The following will wipe the database on startup, so use with caution.

Once the database has been wiped, data node will reconstruct the database tables and will allow you to repopulate the data from the network chain, however this can take a long time depending on the size of the chain.
:::

If you want to wipe the database on startup, you can set the `WipeOnStartup` flag to `true` in the data node configuration file `YOUR_DATA_NODE_HOME_PATH/config/data-node/config.toml`:

```toml
[SQLStore]
  WipeOnStartup = true
```

### Embedded Postgres
:::warning
This is not recommended for use in production, but you can use it to test or learn about the system.
:::

If you do not have access to, or do not want to use a PostgreSQL database server, or a Postgres Docker container, it is possible to run a data node with an embedded version of Postgres. You can enable this by setting the flag:

```toml
[SQLStore]
  ...
  UseEmbedded = true
  ...
```

This will cause data node to download a specially prepared Postgresql package which is extracted to your local machine if it doesn't exist. A separate Postgresql process will be spawned by data node using the credentials you specified in the database configuration section. Once data node is stopped, the child Postgresql process will be stopped automatically.

You can launch Postgresql in its own separate process using the data node embedded postgresql binaries by running the following command:

```shell
vega datanode postgres run --home="YOUR_DATA_NODE_HOME_PATH"
```

In either case, the files for the database will be stored in the data node `state` folder located at `YOUR_DATA_NODE_HOME_PATH/state.data-node/storage/sqlstore`.

### Buffered event source
When a data node is restarted from snapshots, it is possible for the event queue to become flooded causing the Vega core to panic when the event queue is full and stop both the Vega core and data node.

To prevent this, the buffered event source flag is set to true by default. You can confirm this by looking at the following config section:

```toml
[Broker]
  ...
  UseBufferedEventSource = true
  ...
```

## Start Vega and data node
It is recommended to start the data node before starting the Vega node. By default if the `Broker.Socket.Enabled` flag is set to true, the Vega node will attempt to connect to the data node on startup. It will continue to try and connect for one minute before giving up.

To start the data node, run the following command:

```shell
vega datanode start --home="YOUR_DATA_NODE_HOME_PATH"
```

To start Vega, run the following command:

```shell
vega start --home="YOUR_VEGA_HOME_PATH" --tendermint-home="YOUR_TENDERMINT_HOME_PATH"
```

## Fetch network history
After starting a data node, you can load in a segment of network history, if you want your node to have more data than provided by the current height. This is particularly useful if you're running an archive node.

To see how much network history your data node has, run the following command:

```shell
vega datanode network-history show --home="YOUR_DATA_NODE_HOME_PATH"
```

To fetch a network history segment, run the command below. Use the ID of the segment you want (for example, the oldest) followed by the number of blocks prior to the segment's height that you want fetch. `2000` is used in the following example. This will result in all blocks from height 3000 to 5000 being retrieved.

```shell
vega datanode fetch <segment-id-of-segment-at-height-5000> 2000
```

## Configure data node APIs
In order for clients to communicate with data nodes, we expose a set of APIs and methods for reading data.

There are currently three protocols to communicate with the data node APIs:

### gRPC
gRPC is an open source remote procedure call (RPC) system initially developed at Google. In data node the gRPC API features streaming of events in addition to standard procedure calls.

The default port (configurable) for the gRPC API is `3007` and matches the [gRPC protobuf definition](https://github.com/vegaprotocol/vega/tree/develop/protos).

gRPC configurations are defined under the `[Gateway.Node]` section of the data node configuration file `YOUR_DATA_NODE_HOME_PATH/config/data-node/config.toml`:

```toml
  [Gateway.Node]
    Port = 3007
    IP = "0.0.0.0"
```

### GraphQL
[GraphQL ↗](https://graphql.org/) is an open-source data query and manipulation language for APIs, and a runtime for fulfilling queries with existing data, originally developed at Facebook. The [Console ↗](https://github.com/vegaprotocol/frontend-monorepo/tree/develop/apps/trading) uses the GraphQL API to retrieve data including streams of events.

The GraphQL API is defined by a [schema ↗](https://github.com/vegaprotocol/vega/blob/master/datanode/gateway/graphql/schema.graphql). External clients will use this schema to communicate with Vega.

Queries can be tested using the GraphQL playground app which is bundled with a node. The default port (configurable) for the playground app is `3008` accessing this in a web browser will show a web app for testing custom queries, mutations and subscriptions.

The GraphQL default port and other configuration options can be found in the data node configuration file `YOUR_DATA_NODE_HOME_PATH/config/data-node/config.toml` under the `Gateway.GraphQL` section:

```toml
  [Gateway.GraphQL]
    Port = 3008
    IP = "0.0.0.0"
    Enabled = true
    ComplexityLimit = 0
    HTTPSEnabled = false
    AutoCertDomain = ""
    CertificateFile = ""
    KeyFile = ""
    Endpoint = "/graphql"
```

#### GraphQL SSL
**GraphQL subscriptions do not work properly unless the HTTPS is enabled**.

To enable TLS on the GraphQL port, set

```toml
  [Gateway.GraphQL]
    HTTPSEnabled = true
```

You will need your data node to be reachable over the internet with a proper fully qualified domain name, and a matching certificate. If you already have a certificate and corresponding private key file, you can specify them as follows:

```toml
  [Gateway.GraphQL]
    CertificateFile = "/path/to/certificate/file"
    KeyFile = "/path/to/key/file"
```

If you prefer, the data node can manage this for you by automatically generating a certificate and using `LetsEncrypt` to sign it for you.

```toml
  [Gateway.GraphQL]
    HTTPSEnabled = true
    AutoCertDomain = "my.lovely.domain.com"
```

However, it is a requirement of the `LetsEncrypt` validation process that the the server answering its challenge is running on the standard HTTPS port (443). This means you must either

- Forward port 443 on your machine to the GraphQL port (3008 by default) using `iptables` or similar
- Directly use port 443 for the GraphQL server in data node by specifying

```toml
  [Gateway.GraphQL]
    Port = 443
```

Note that Linux systems generally require processes listening on ports under 1024 to either

- run as root, or
- be specifically granted permission, e.g. by launching with

```shell
setcap cap_net_bind_service=ep vega datanode run
```

#### GraphQL complexity
Currently the GraphQL complexity limit is globally set to 3750.

This setting is theoretical at the moment and will be refined and have different levels for different queries/resolvers in the future.

The intention behind this limit is to prevent the VEGA system from being abused by heavy queries (DOS). The complexity level is mostly affected by the number of objects a query contains. So the heaviest ones we currently have in the system are:

| Query | Items | Complexity |
| ----- | ----- | ---------- |
| SimpleMarkets (embedded candles) | 1 candle | 151 |
| SimpleMarkets (embedded candles) | 91 candle | 788 |
| MarketInfo (embedded candles) | 1 candle | 399 |
| MarketInfo (embedded candles) | 91 candles | 1036 |
| Orders (embedded orders) | 1 order | 163 |
| Orders (embedded orders) | 80 orders | 4003 |
| Trades (embedded trades) | 1 trades | 118 |
| Trades (embedded trades) | 75 trades | 1393 |
| Positions (embedded positions) | 1 position | 129 |
| Positions (embedded positions) | 40 positions | 2500 |

The approximate number of positions queries by customers is 40.

The GraphQL will return error for queries that have complexity above the set limit: "GraphQL error: Query is too complex to execute" and will not proceed with execution.

### REST
REST provides a standard between computer systems on the web, making it easier for systems to communicate with each other. It is arguably simpler to work with than gRPC and GraphQL. In Vega the REST API is a reverse proxy to the gRPC API, however it does not support streaming.

The default port (configurable) for the REST API is `3009` and we use a reverse proxy to the gRPC API to deliver the REST API implementation.

## Further reading
For more information about data node and developing on data node please see the data node [README ↗](https://github.com/vegaprotocol/vega/blob/master/datanode/README.md)
